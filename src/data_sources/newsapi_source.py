"""
NewsAPI ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import requests
import os
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Optional

from src.models import UniversalArticle


class NewsAPISource:
    """
    NewsAPI ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        NewsAPISource ã‚’åˆæœŸåŒ–

        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            api_key (Optional[str]): NewsAPI ã‚­ãƒ¼ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        """
        self.api_key = api_key or os.getenv('NEWSAPI_KEY')
        if not self.api_key:
            raise ValueError("NEWSAPI_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.env ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

        self.base_url = 'https://newsapi.org/v2/everything'

    def fetch_articles(
        self,
        keyword: str,
        language: str = 'ja',
        page_size: int = 20,
        sort_by: str = 'publishedAt'
    ) -> List[Dict]:
        """
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ¤œç´¢

        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            keyword (str): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: "AI", "Python"ï¼‰
            language (str): è¨€èªã‚³ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'ja' - æ—¥æœ¬èªï¼‰
            page_size (int): å–å¾—ã™ã‚‹è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ã€æœ€å¤§: 100ï¼‰
            sort_by (str): ã‚½ãƒ¼ãƒˆé †ï¼ˆ'publishedAt', 'relevancy', 'popularity'ï¼‰

        æˆ»ã‚Šå€¤:
            List[Dict]: è¨˜äº‹ã®ãƒªã‚¹ãƒˆã€‚å„è¨˜äº‹ã¯è¾æ›¸å½¢å¼ã€‚

        ä¾‹å¤–:
            requests.exceptions.RequestException: API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¤±æ•—ã—ãŸå ´åˆ
        """

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if not keyword or not keyword.strip():
            raise ValueError("keyword ã¯ç©ºã«ã§ãã¾ã›ã‚“")

        if page_size < 1 or page_size > 100:
            raise ValueError("page_size ã¯ 1ã€œ100 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„")

        # API ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            'q': keyword,
            'language': language,
            'pageSize': page_size,
            'sortBy': sort_by,
            'apiKey': self.api_key
        }

        try:
            print(f"ğŸ“¡ NewsAPI ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰='{keyword}', è¨€èª={language}")

            response = requests.get(self.base_url, params=params, timeout=10)
            response.raise_for_status()  # HTTP ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ

            data = response.json()

            # API ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
            if data.get('status') != 'ok':
                error_message = data.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
                raise Exception(f"NewsAPI ã‚¨ãƒ©ãƒ¼: {error_message}")

            articles = data.get('articles', [])
            total_results = data.get('totalResults', 0)

            print(f"âœ… {keyword}: {len(articles)} ä»¶å–å¾—ï¼ˆå…¨ {total_results} ä»¶ä¸­ï¼‰")

            return articles

        except requests.exceptions.Timeout:
            print(f"âŒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: NewsAPI ã¸ã®æ¥ç¶šãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
            raise

        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else 'unknown'
            print(f"âŒ HTTP ã‚¨ãƒ©ãƒ¼ ({status_code}): {e}")
            raise

        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def fetch_top_headlines(
        self,
        country: str = 'jp',
        category: Optional[str] = None,
        page_size: int = 20
    ) -> List[Dict]:
        """
        ãƒˆãƒƒãƒ—ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³ã‚’å–å¾—ï¼ˆå›½åˆ¥ï¼‰

        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            country (str): å›½ã‚³ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'jp' - æ—¥æœ¬ï¼‰
            category (Optional[str]): ã‚«ãƒ†ã‚´ãƒªï¼ˆ'business', 'technology', 'science' ãªã©ï¼‰
            page_size (int): å–å¾—ã™ã‚‹è¨˜äº‹æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰

        æˆ»ã‚Šå€¤:
            List[Dict]: è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """

        url = 'https://newsapi.org/v2/top-headlines'

        params = {
            'country': country,
            'pageSize': page_size,
            'apiKey': self.api_key
        }

        if category:
            params['category'] = category

        try:
            print(f"ğŸ“¡ ãƒˆãƒƒãƒ—ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³å–å¾—ä¸­: å›½={country}")

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()

            if data.get('status') != 'ok':
                error_message = data.get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
                raise Exception(f"NewsAPI ã‚¨ãƒ©ãƒ¼: {error_message}")

            articles = data.get('articles', [])

            print(f"âœ… ãƒˆãƒƒãƒ—ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³: {len(articles)} ä»¶å–å¾—")

            return articles

        except requests.exceptions.RequestException as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    @staticmethod
    def normalize(newsapi_article: Dict) -> UniversalArticle:
        """
        NewsAPI ã®è¨˜äº‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ UniversalArticle ã«å¤‰æ›

        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            newsapi_article (dict): NewsAPI ã‹ã‚‰è¿”ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹

        æˆ»ã‚Šå€¤:
            UniversalArticle: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã•ã‚ŒãŸè¨˜äº‹

        ä¾‹:
            raw_article = {
                'source': {'name': 'TechCrunch'},
                'title': 'OpenAI releases GPT-5',
                'url': 'https://...',
                'publishedAt': '2026-01-29T08:00:00Z',
                'description': '...',
                'content': '...'
            }
            article = NewsAPISource.normalize(raw_article)
        """

        # ã‚½ãƒ¼ã‚¹åã‚’å–å¾—
        source_name = newsapi_article.get('source', {}).get('name', 'Unknown')

        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = newsapi_article.get('title', 'Untitled')

        # å…¬é–‹æ—¥ã‚’ datetime ã«å¤‰æ›
        published_at_str = newsapi_article.get('publishedAt', '')
        try:
            if published_at_str:
                # ISO 8601 å½¢å¼ã®æ–‡å­—åˆ—ã‚’ datetime ã«å¤‰æ›
                # 'Z' ã‚’ '+00:00' ã«ç½®ãæ›ãˆã¦ UTC ã¨ã—ã¦æ‰±ã†
                published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
            else:
                # å…¬é–‹æ—¥ãŒãªã„å ´åˆã¯ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
                published_at = datetime.now(timezone.utc)
        except (ValueError, TypeError):
            # ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã‚‚ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨
            published_at = datetime.now(timezone.utc)

        # å–å¾—æ™‚åˆ»ï¼ˆUTCï¼‰
        fetched_at = datetime.now(timezone.utc)

        # ä¸€æ„ ID ã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + ã‚½ãƒ¼ã‚¹åã®ãƒãƒƒã‚·ãƒ¥ï¼‰
        # åŒã˜è¨˜äº‹ãªã‚‰åŒã˜IDã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹
        article_id = str(uuid.uuid5(
            uuid.NAMESPACE_DNS,
            f"{title}-{source_name}"
        ))

        # è¨€èªã‚’åˆ¤å®šï¼ˆç°¡æ˜“çš„ï¼‰
        description = newsapi_article.get('description', '')
        content = newsapi_article.get('content', '')
        combined_text = f"{title} {description}"

        # æ—¥æœ¬èªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        has_japanese = any('\u3000' <= c <= '\u9fff' or '\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff'
                          for c in combined_text)
        language = 'ja' if has_japanese else 'en'

        # UniversalArticle ã«å¤‰æ›
        return UniversalArticle(
            id=article_id,
            title=title,
            source_url=newsapi_article.get('url', ''),
            source_name=source_name,
            published_at=published_at,
            fetched_at=fetched_at,
            source_type='newsapi',
            category='unknown',  # å¾Œã§ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šæ©Ÿèƒ½ã‚’è¿½åŠ 
            description=description,
            content=content,
            image_url=newsapi_article.get('urlToImage'),
            language=language,
            original_data=newsapi_article  # å…ƒãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒã—ã¦ãŠã
        )

    @classmethod
    def fetch_and_normalize(
        cls,
        keyword: str,
        language: str = 'ja',
        page_size: int = 20,
        api_key: Optional[str] = None
    ) -> List[UniversalArticle]:
        """
        NewsAPI ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã—ã€UniversalArticle ã«å¤‰æ›ã™ã‚‹ä¾¿åˆ©ãƒ¡ã‚½ãƒƒãƒ‰

        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
            keyword (str): æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            language (str): è¨€èªã‚³ãƒ¼ãƒ‰
            page_size (int): å–å¾—ã™ã‚‹è¨˜äº‹æ•°
            api_key (Optional[str]): API ã‚­ãƒ¼

        æˆ»ã‚Šå€¤:
            List[UniversalArticle]: æ­£è¦åŒ–ã•ã‚ŒãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆ
        """
        source = cls(api_key=api_key)
        raw_articles = source.fetch_articles(keyword, language=language, page_size=page_size)

        # å„è¨˜äº‹ã‚’ UniversalArticle ã«å¤‰æ›
        normalized_articles = [cls.normalize(article) for article in raw_articles]

        return normalized_articles
