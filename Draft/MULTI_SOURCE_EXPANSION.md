# Multi Source Expansion - ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ‹¡å¼µã‚¬ã‚¤ãƒ‰

## ğŸ¯ ç›®çš„

æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ï¼ˆRSSã€EDINETã€arXiv ãªã©ï¼‰ã‚’è¿½åŠ ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã€‚

---

## ğŸ“ æ–°ã—ã„ã‚½ãƒ¼ã‚¹è¿½åŠ ã® 5 ã‚¹ãƒ†ãƒƒãƒ—

### Step 1: ã‚½ãƒ¼ã‚¹ç”¨ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿

```python
# src/data_sources/new_source.py

from src.data_sources.base_source import BaseSource
from src.models import UniversalArticle

class NewSource(BaseSource):
    """
    æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹
    """
    
    def __init__(self):
        self.source_type = "new_source"
        self.source_name = "New Source Name"
    
    async def fetch(self) -> list:
        """
        ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ï¼ˆå®Ÿè£…å¿…é ˆï¼‰
        """
        # API å‘¼ã³å‡ºã—ã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç­‰
        pass
    
    def normalize(self, raw_data: dict) -> UniversalArticle:
        """
        å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ UniversalArticle ã«æ­£è¦åŒ–
        """
        return UniversalArticle(
            id=raw_data.get('id'),
            title=raw_data.get('title'),
            source_url=raw_data.get('url'),
            source_name=self.source_name,
            published_at=parse_date(raw_data.get('published')),
            fetched_at=datetime.now(),
            source_type=self.source_type,
            original_data=raw_data
        )
```

### Step 2: ãƒ¡ã‚¤ãƒ³å‡¦ç†ã«ç™»éŒ²

```python
# src/main.py

async def main():
    from src.data_sources.newsapi_source import NewsAPISource
    from src.data_sources.new_source import NewSource
    
    fetchers = [
        NewsAPISource(),
        NewSource(),  # â† æ–°ã—ã„ã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ 
    ]
    
    all_articles = []
    for fetcher in fetchers:
        articles = await fetcher.fetch()
        all_articles.extend(articles)
```

### Step 3: è¨­å®šã«è¿½åŠ 

```python
# config.py

DATA_SOURCES = {
    "newsapi": {
        "enabled": True,
        "categories": ["AI", "æ±ºç®—", "ç§‘å­¦"],
        "weight": 1.0
    },
    "new_source": {
        "enabled": True,
        "categories": ["ç§‘å­¦"],  # ã“ã®ã‚½ãƒ¼ã‚¹ã¯ã©ã®ã‚«ãƒ†ã‚´ãƒªå¯¾è±¡ã‹
        "weight": 1.0,
        "rate_limit": {"calls": 100, "window": 3600}
    }
}
```

### Step 4: ãƒ†ã‚¹ãƒˆå®Ÿè£…

```python
# tests/test_new_source.py

@pytest.mark.asyncio
async def test_new_source_fetch():
    source = NewSource()
    articles = await source.fetch()
    
    assert len(articles) > 0
    assert all(hasattr(a, 'title') for a in articles)
```

### Step 5: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°

README.md ã«è¿½åŠ ï¼š
```
### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚½ãƒ¼ã‚¹
- NewsAPI
- New Sourceï¼ˆèª¬æ˜ï¼‰
```

---

## ğŸ’¡ å®Ÿè£…ä¾‹ 1ï¼šRSS ãƒ•ã‚£ãƒ¼ãƒ‰è¿½åŠ 

```python
# src/data_sources/rss_source.py

import feedparser

class RSSSource(BaseSource):
    
    def __init__(self, url: str, source_name: str):
        self.url = url
        self.source_name = source_name
        self.source_type = "rss"
    
    async def fetch(self) -> list:
        """
        RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
        """
        feed = feedparser.parse(self.url)
        articles = []
        
        for entry in feed.entries[:10]:  # æœ€æ–° 10 ä»¶
            article = self.normalize(entry)
            articles.append(article)
        
        return articles
```

---

## ğŸ’¡ å®Ÿè£…ä¾‹ 2ï¼šEDINETï¼ˆæ±ºç®—æƒ…å ±ï¼‰è¿½åŠ 

```python
# src/data_sources/edinet_source.py

import requests

class EDINETSource(BaseSource):
    
    def __init__(self):
        self.api_url = "https://api.edinet-fsa.go.jp"
        self.source_type = "edinet"
    
    async def fetch(self) -> list:
        """
        EDINET ã‹ã‚‰ä¼æ¥­æ±ºç®—æƒ…å ±ã‚’å–å¾—
        """
        params = {
            'date': '2026-01-29',  # æœ¬æ—¥ã®æ±ºç®—ç™ºè¡¨
            'type': 120  # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸
        }
        
        response = requests.get(f"{self.api_url}/documents", params=params)
        data = response.json()
        
        articles = [
            self.normalize(filing)
            for filing in data['results']
        ]
        
        return articles
```

---

## ğŸ”— data_fetcher ã§è¤‡æ•°ã‚½ãƒ¼ã‚¹çµ±åˆ

```python
# src/data_fetcher.py

class DataFetcher:
    """
    è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã¦å–å¾—
    """
    
    def __init__(self):
        # ã‚½ãƒ¼ã‚¹ã‚’ç™»éŒ²
        self.sources = {
            'newsapi': NewsAPISource(),
            'rss_nikkei': RSSSource(
                'https://www.nikkei.com/rss/',
                'Nikkei'
            ),
            'rss_nhk': RSSSource(
                'https://www.nhk.or.jp/rss/',
                'NHK'
            ),
            'edinet': EDINETSource(),
            'arxiv': ArxivSource()
        }
    
    async def fetch_all(self) -> list:
        """
        å…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰éåŒæœŸã§å–å¾—
        """
        tasks = [
            self._fetch_with_rate_limit(name, source)
            for name, source in self.sources.items()
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_articles = []
        for result in results:
            if isinstance(result, Exception):
                logging.error(f"Fetch error: {result}")
            else:
                all_articles.extend(result)
        
        return all_articles
```

---

## ğŸ“Š ã‚½ãƒ¼ã‚¹ç®¡ç†

```python
# ã‚½ãƒ¼ã‚¹ã”ã¨ã®ä¿¡é ¼åº¦ãƒ»å„ªå…ˆåº¦è¨­å®š

SOURCE_CONFIG = {
    "newsapi": {
        "credibility": 12,
        "priority": "high",
        "cost": "free"
    },
    "rss_nikkei": {
        "credibility": 18,
        "priority": "high",
        "cost": "free"
    },
    "edinet": {
        "credibility": 20,
        "priority": "critical",
        "cost": "free"
    },
    "arxiv": {
        "credibility": 18,
        "priority": "medium",
        "cost": "free"
    }
}
```

---

## âœ… æ–°ã‚½ãƒ¼ã‚¹è¿½åŠ ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] ã‚½ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹å®Ÿè£…ï¼ˆBaseSource ç¶™æ‰¿ï¼‰
- [ ] fetch() ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
- [ ] normalize() ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
- [ ] config.py ã«ç™»éŒ²
- [ ] ãƒ†ã‚¹ãƒˆå®Ÿè£…
- [ ] data_fetcher ã«ç™»éŒ²
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
- [ ] pytest ã§ãƒ†ã‚¹ãƒˆæˆåŠŸ

---

**æ¬¡ã¯ GITHUB_ACTIONS_SETUP.md ã‚’èª­ã‚“ã§ãã ã•ã„ï¼**
