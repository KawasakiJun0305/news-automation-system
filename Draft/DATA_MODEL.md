# Data Model - ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆæ›¸

## ğŸ“š æ¦‚è¦

è¤‡æ•°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ï¼ˆNewsAPI, RSS, EDINET, arXiv ãªã©ï¼‰ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã€
**çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–** ã™ã‚‹ã“ã¨ã§ã€ä»¥é™ã®å‡¦ç†ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ç­‰ï¼‰
ã‚’çµ±ä¸€çš„ã«å®Ÿè£…ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

---

## ğŸ¯ çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒï¼šUniversalArticle ã‚¯ãƒ©ã‚¹

ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‚’ã€ã“ã®çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚

```python
# src/models.py

from dataclasses import dataclass
from datetime import datetime
from typing import Optional, List
import uuid

@dataclass
class UniversalArticle:
    """
    å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®çµ±ä¸€ã‚¹ã‚­ãƒ¼ãƒ
    
    ã©ã®ã‚½ãƒ¼ã‚¹ï¼ˆNewsAPI, RSS, EDINET ãªã©ï¼‰ã‹ã‚‰å–å¾—ã—ãŸè¨˜äº‹ã§ã‚‚ã€
    ã“ã®ã‚¯ãƒ©ã‚¹ã«çµ±ä¸€ã—ã¦æ‰±ã†ã€‚
    
    ã€ä½¿ç”¨ä¾‹ã€‘
    article = UniversalArticle(
        id="uuid-xxx",
        title="OpenAI ãŒ GPT-5 ã‚’ç™ºè¡¨",
        source_url="https://example.com",
        ...
    )
    """
    
    # =====================================
    # ã€ã‚³ã‚¢æƒ…å ±ã€‘å…¨ã‚½ãƒ¼ã‚¹ã§å¿…é ˆ
    # =====================================
    id: str                           # ä¸€æ„è­˜åˆ¥å­ï¼ˆUUIDï¼‰
    title: str                        # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
    source_url: str                   # å…ƒè¨˜äº‹ã¸ã®ãƒªãƒ³ã‚¯
    source_name: str                  # ã‚½ãƒ¼ã‚¹åï¼ˆ"NewsAPI", "æ—¥çµŒæ–°è" ãªã©ï¼‰
    published_at: datetime            # è¨˜äº‹ã®å…¬é–‹æ—¥æ™‚
    fetched_at: datetime              # ã‚·ã‚¹ãƒ†ãƒ ãŒè¨˜äº‹ã‚’å–å¾—ã—ãŸæ—¥æ™‚
    
    # =====================================
    # ã€åˆ†é¡æƒ…å ±ã€‘ã‚«ãƒ†ã‚´ãƒªãƒ»ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—
    # =====================================
    source_type: str                  # ã‚½ãƒ¼ã‚¹ã®ç¨®é¡
                                      # "newsapi" | "rss" | "edinet" | "arxiv"
    category: str                     # ã‚«ãƒ†ã‚´ãƒªï¼ˆå¾Œã§è‡ªå‹•åˆ¤å®šï¼‰
                                      # "AI" | "æ±ºç®—" | "ç§‘å­¦" | "ãƒ¢ãƒã¥ãã‚Š" | "ãƒœãƒ¼ãƒ‰ã‚²ãƒ¼ãƒ "
    
    # =====================================
    # ã€å‡¦ç†å¾Œã®æƒ…å ±ã€‘ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆNone å¯ï¼‰
    # =====================================
    summary: Optional[str] = None     # è¦ç´„ï¼ˆClaude ç”Ÿæˆï¼‰
    keywords: Optional[List[str]] = None  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè‡ªå‹•æŠ½å‡ºï¼‰
    relevance_score: Optional[int] = None # é–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
    credibility_score: Optional[int] = None # ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼ˆ0-100ï¼‰
    
    # =====================================
    # ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‘å…ƒã®ãƒ‡ãƒ¼ã‚¿ä¿æŒç”¨
    # =====================================
    original_data: dict = None        # å…ƒã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå‚ç…§ç”¨ï¼‰
                                      # ãªãœï¼Ÿå„ã‚½ãƒ¼ã‚¹å›ºæœ‰ã®æƒ…å ±ãŒå¿…è¦ãªæ™‚ãŒã‚ã‚‹ãŸã‚
    
    # =====================================
    # ã€ã‚½ãƒ¼ã‚¹å›ºæœ‰æƒ…å ±ã€‘ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    # =====================================
    authors: Optional[List[str]] = None     # è‘—è€…ï¼ˆè«–æ–‡ã®å ´åˆãªã©ï¼‰
    language: str = "ja"                    # è¨€èªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šæ—¥æœ¬èªï¼‰
    region: str = "JP"                      # åœ°åŸŸï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šæ—¥æœ¬ï¼‰
    
    # =====================================
    # ã€ãã®ä»–ãƒ•ãƒ©ã‚°ã€‘
    # =====================================
    is_cached: bool = False           # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ãŸã‹ï¼Ÿ
    is_duplicate: bool = False        # é‡è¤‡æ¤œå‡ºæ¸ˆã¿ã‹ï¼Ÿ
    
    def __post_init__(self):
        """
        ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å‡¦ç†
        
        ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„å½¢å¼ã‹ç¢ºèªã€‚
        ä¾‹ãˆã°ã€relevance_score ã¯ 0-100 ã®ç¯„å›²ã‹ï¼Ÿãªã©
        """
        if self.relevance_score is not None:
            assert 0 <= self.relevance_score <= 100, \
                "relevance_score ã¯ 0-100 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„"
        
        if self.credibility_score is not None:
            assert 0 <= self.credibility_score <= 100, \
                "credibility_score ã¯ 0-100 ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„"
```

---

## ğŸ”„ å„ã‚½ãƒ¼ã‚¹ã®æ­£è¦åŒ–æ–¹æ³•

### **1. NewsAPI â†’ UniversalArticle**

**NewsAPI ã®å¿œç­”ä¾‹ï¼š**
```json
{
  "source": {
    "id": "techcrunch",
    "name": "TechCrunch"
  },
  "author": "Sarah Chen",
  "title": "OpenAI Releases GPT-5 Preview",
  "description": "OpenAI announced GPT-5...",
  "url": "https://techcrunch.com/...",
  "urlToImage": "https://...",
  "publishedAt": "2026-01-29T08:00:00Z",
  "content": "OpenAI announced..."
}
```

**å¤‰æ›ã‚³ãƒ¼ãƒ‰ï¼š**
```python
# src/data_sources/newsapi_source.py

from src.models import UniversalArticle
from datetime import datetime
import uuid

class NewsAPISource:
    
    @staticmethod
    def normalize(newsapi_article: dict) -> UniversalArticle:
        """
        NewsAPI ã®è¨˜äº‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ UniversalArticle ã«å¤‰æ›
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š
            newsapi_article (dict): NewsAPI ã‹ã‚‰è¿”ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹
        
        æˆ»ã‚Šå€¤ï¼š
            UniversalArticle: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã•ã‚ŒãŸè¨˜äº‹
        """
        
        # ã‚½ãƒ¼ã‚¹åã‚’å–å¾—ï¼ˆè¾æ›¸ã®ã‚­ãƒ¼åãŒ "source" ã§ã€ãã®ä¸­ã« "name" ãŒã‚ã‚‹ï¼‰
        source_name = newsapi_article.get('source', {}).get('name', 'Unknown')
        
        # å…¬é–‹æ—¥ã‚’ datetime ã«å¤‰æ›
        published_at_str = newsapi_article.get('publishedAt', '')
        published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
        
        # ä¸€æ„ ID ã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ« + ã‚½ãƒ¼ã‚¹åã®ãƒãƒƒã‚·ãƒ¥ï¼‰
        article_id = str(uuid.uuid5(
            uuid.NAMESPACE_DNS,
            f"{newsapi_article['title']}-{source_name}"
        ))
        
        # UniversalArticle ã«å¤‰æ›
        return UniversalArticle(
            id=article_id,
            title=newsapi_article.get('title', ''),
            source_url=newsapi_article.get('url', ''),
            source_name=source_name,
            published_at=published_at,
            fetched_at=datetime.now(timezone.utc),
            source_type='newsapi',
            category='unknown',  # å¾Œã§è‡ªå‹•åˆ¤å®š
            original_data=newsapi_article  # å…ƒãƒ‡ãƒ¼ã‚¿ã¯ä¿æŒã—ã¦ãŠã
        )
```

---

### **2. RSS ãƒ•ã‚£ãƒ¼ãƒ‰ â†’ UniversalArticle**

**RSS ã‚¨ãƒ³ãƒˆãƒªã®ä¾‹ï¼š**
```xml
<item>
  <title>æ—¥çµŒæ–°èï¼šãƒˆãƒ¨ã‚¿ã®å–¶æ¥­åˆ©ç›ŠãŒéå»æœ€é«˜</title>
  <link>https://nikkei.com/article/...</link>
  <pubDate>Wed, 29 Jan 2026 12:00:00 +0900</pubDate>
  <description>ãƒˆãƒ¨ã‚¿è‡ªå‹•è»ŠãŒ...</description>
</item>
```

**å¤‰æ›ã‚³ãƒ¼ãƒ‰ï¼š**
```python
# src/data_sources/rss_source.py

import feedparser
from email.utils import parsedate_to_datetime
import uuid

class RSSSource:
    
    @staticmethod
    def normalize(rss_entry: dict, source_name: str) -> UniversalArticle:
        """
        RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ UniversalArticle ã«å¤‰æ›
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š
            rss_entry (dict): feedparser ãŒè§£æã—ãŸ RSS ã‚¨ãƒ³ãƒˆãƒª
            source_name (str): RSS ã‚½ãƒ¼ã‚¹ã®åå‰ï¼ˆ"æ—¥çµŒæ–°è" ãªã©ï¼‰
        
        æˆ»ã‚Šå€¤ï¼š
            UniversalArticle: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        
        # å…¬é–‹æ—¥ã‚’ datetime ã«å¤‰æ›
        # RSS ã§ã¯ "published_parsed" ãŒ struct_time ãªã®ã§ datetime ã«å¤‰æ›
        published_at = datetime(*rss_entry['published_parsed'][:6])
        
        # ä¸€æ„ ID ã‚’ç”Ÿæˆ
        article_id = str(uuid.uuid5(
            uuid.NAMESPACE_DNS,
            f"{rss_entry['title']}-{source_name}"
        ))
        
        return UniversalArticle(
            id=article_id,
            title=rss_entry.get('title', ''),
            source_url=rss_entry.get('link', ''),
            source_name=source_name,
            published_at=published_at,
            fetched_at=datetime.now(timezone.utc),
            source_type='rss',
            category='unknown',
            original_data=rss_entry
        )
```

---

### **3. EDINETï¼ˆæ—¥æœ¬ä¼æ¥­æ±ºç®—ï¼‰â†’ UniversalArticle**

**EDINET é–‹ç¤ºæƒ…å ±ã®ä¾‹ï¼š**
```json
{
  "document_id": "S100ABC123",
  "document_name": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ 2025å¹´åº¦ç¬¬3å››åŠæœŸæ±ºç®—èª¬æ˜ä¼šè³‡æ–™",
  "submitter_name": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Š",
  "submission_date": "2026-01-28",
  "document_url": "https://edinet-api.fsa.go.jp/doc/S100ABC123",
  "xbrl_url": "https://edinet-api.fsa.go.jp/xbrl/S100ABC123"
}
```

**å¤‰æ›ã‚³ãƒ¼ãƒ‰ï¼š**
```python
# src/data_sources/edinet_source.py

import uuid

class EDINETSource:
    
    @staticmethod
    def normalize(filing_data: dict) -> UniversalArticle:
        """
        EDINET é–‹ç¤ºæƒ…å ±ã‚’ UniversalArticle ã«å¤‰æ›
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š
            filing_data (dict): EDINET API ã‹ã‚‰è¿”ã•ã‚ŒãŸé–‹ç¤ºæƒ…å ±
        
        æˆ»ã‚Šå€¤ï¼š
            UniversalArticle: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        
        # æå‡ºæ—¥ã‚’ datetime ã«å¤‰æ›
        submission_date_str = filing_data.get('submission_date', '')
        published_at = datetime.strptime(submission_date_str, '%Y-%m-%d')
        
        # ä¸€æ„ ID ã‚’ç”Ÿæˆ
        article_id = filing_data.get('document_id', str(uuid.uuid4()))
        
        return UniversalArticle(
            id=article_id,
            title=filing_data.get('document_name', ''),
            source_url=filing_data.get('document_url', ''),
            source_name=filing_data.get('submitter_name', ''),
            published_at=published_at,
            fetched_at=datetime.now(timezone.utc),
            source_type='edinet',
            category='æ±ºç®—',  # EDINET ã¯å¿…ãšæ±ºç®—é–¢é€£
            original_data=filing_data
        )
```

---

### **4. arXivï¼ˆå­¦è¡“è«–æ–‡ï¼‰â†’ UniversalArticle**

**arXiv API ã®å¿œç­”ä¾‹ï¼š**
```json
{
  "id": "2401.12345",
  "title": "A Novel Approach to Quantum Computing",
  "authors": ["Alice Smith", "Bob Johnson"],
  "summary": "We propose a new method for...",
  "published": "2026-01-29T10:00:00Z",
  "arxiv_url": "https://arxiv.org/abs/2401.12345"
}
```

**å¤‰æ›ã‚³ãƒ¼ãƒ‰ï¼š**
```python
# src/data_sources/arxiv_source.py

import uuid

class ArxivSource:
    
    @staticmethod
    def normalize(paper: dict) -> UniversalArticle:
        """
        arXiv è«–æ–‡æƒ…å ±ã‚’ UniversalArticle ã«å¤‰æ›
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š
            paper (dict): arXiv API ã‹ã‚‰è¿”ã•ã‚ŒãŸè«–æ–‡æƒ…å ±
        
        æˆ»ã‚Šå€¤ï¼š
            UniversalArticle: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        """
        
        # å…¬é–‹æ—¥ã‚’ datetime ã«å¤‰æ›
        published_at = datetime.fromisoformat(
            paper.get('published', '').replace('Z', '+00:00')
        )
        
        # è‘—è€…ãƒªã‚¹ãƒˆã‚’å–å¾—
        authors = paper.get('authors', [])
        
        return UniversalArticle(
            id=paper.get('id', str(uuid.uuid4())),
            title=paper.get('title', ''),
            source_url=paper.get('arxiv_url', ''),
            source_name='arXiv',
            published_at=published_at,
            fetched_at=datetime.now(timezone.utc),
            source_type='arxiv',
            category='ç§‘å­¦',  # arXiv ã¯å¿…ãšç§‘å­¦é–¢é€£
            authors=authors,
            original_data=paper
        )
```

---

## ğŸ”— æ­£è¦åŒ–ã®æµã‚Œï¼ˆå…¨ä½“ï¼‰

```python
# src/normalizer.py

from src.data_sources.newsapi_source import NewsAPISource
from src.data_sources.rss_source import RSSSource
from src.data_sources.edinet_source import EDINETSource
from src.data_sources.arxiv_source import ArxivSource

class DataNormalizer:
    """
    è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã€
    UniversalArticle ã«çµ±ä¸€ã™ã‚‹å‡¦ç†
    """
    
    @staticmethod
    def normalize_articles(raw_articles_by_source: dict) -> List[UniversalArticle]:
        """
        è¤‡æ•°ã‚½ãƒ¼ã‚¹ã®è¨˜äº‹ã‚’ã¾ã¨ã‚ã¦æ­£è¦åŒ–
        
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼š
            raw_articles_by_source (dict): {
                'newsapi': [è¨˜äº‹, è¨˜äº‹, ...],
                'rss': [è¨˜äº‹, è¨˜äº‹, ...],
                'edinet': [è¨˜äº‹, è¨˜äº‹, ...]
            }
        
        æˆ»ã‚Šå€¤ï¼š
            List[UniversalArticle]: çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        normalized = []
        
        # NewsAPI è¨˜äº‹ã‚’æ­£è¦åŒ–
        for article in raw_articles_by_source.get('newsapi', []):
            normalized.append(NewsAPISource.normalize(article))
        
        # RSS è¨˜äº‹ã‚’æ­£è¦åŒ–
        for source_name, entries in raw_articles_by_source.get('rss', {}).items():
            for entry in entries:
                normalized.append(RSSSource.normalize(entry, source_name))
        
        # EDINET è¨˜äº‹ã‚’æ­£è¦åŒ–
        for filing in raw_articles_by_source.get('edinet', []):
            normalized.append(EDINETSource.normalize(filing))
        
        # arXiv è«–æ–‡ã‚’æ­£è¦åŒ–
        for paper in raw_articles_by_source.get('arxiv', []):
            normalized.append(ArxivSource.normalize(paper))
        
        return normalized
```

---

## ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ

### **ãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ï¼ˆSQLite ä¾‹ï¼‰**

```sql
-- articles_history ãƒ†ãƒ¼ãƒ–ãƒ«
-- æ¯æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è“„ç©ã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«

CREATE TABLE articles_history (
    -- åŸºæœ¬æƒ…å ±
    id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    source_url TEXT,
    source_name TEXT,
    source_type TEXT,
    
    -- æ—¥æ™‚æƒ…å ±
    published_at DATETIME,
    fetched_at DATETIME,
    
    -- åˆ†é¡
    category TEXT,
    
    -- ã‚¹ã‚³ã‚¢
    relevance_score INTEGER,
    credibility_score INTEGER,
    
    -- å‡¦ç†æ¸ˆã¿æƒ…å ±
    summary TEXT,
    keywords TEXT,  -- JSON å½¢å¼ã§ä¿å­˜ï¼š["AI", "ä¼æ¥­"]
    
    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    authors TEXT,   -- JSON å½¢å¼
    original_data TEXT,  -- JSON å½¢å¼ï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ï¼‰
    
    -- ãƒ•ãƒ©ã‚°
    is_cached BOOLEAN DEFAULT 0,
    is_duplicate BOOLEAN DEFAULT 0,
    
    -- ãƒ¬ã‚³ãƒ¼ãƒ‰ç®¡ç†
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ¤œç´¢ã‚’é«˜é€ŸåŒ–ï¼‰
CREATE INDEX idx_category ON articles_history(category);
CREATE INDEX idx_published_at ON articles_history(published_at);
CREATE INDEX idx_source_type ON articles_history(source_type);
```

### **CSV ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**

```csv
id,title,source_name,published_at,fetched_at,category,relevance_score,summary,keywords
abc123,OpenAI ãŒ GPT-5 ã‚’ç™ºè¡¨,TechCrunch,2026-01-29T08:00:00,2026-01-29T06:15:00,AI,92,OpenAI ã¯æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ« GPT-5 ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚ç²¾åº¦ãŒ 35% å‘ä¸Šã—ã¦ã„ã¾ã™ã€‚,"[""AI"",""OpenAI"",""è¨€èªãƒ¢ãƒ‡ãƒ«""]"
def456,ãƒˆãƒ¨ã‚¿å–¶æ¥­åˆ©ç›Š 28% å¢—,EDINET,2026-01-28T09:00:00,2026-01-29T06:15:00,æ±ºç®—,85,ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šã®å–¶æ¥­åˆ©ç›ŠãŒéå»æœ€é«˜ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚EV äº‹æ¥­ãŒå¥½èª¿ã§ã™ã€‚,"[""æ±ºç®—"",""ãƒˆãƒ¨ã‚¿"",""è‡ªå‹•è»Š""]"
```

---

## ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

```python
# src/models.py

class UniversalArticle:
    
    def validate(self) -> bool:
        """
        è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ã„ã‹æ¤œè¨¼
        
        æˆ»ã‚Šå€¤ï¼š
            bool: ãƒ‡ãƒ¼ã‚¿ãŒæœ‰åŠ¹ãªã‚‰ Trueã€ç„¡åŠ¹ãªã‚‰ False
        """
        
        # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
        if not self.id or not self.title or not self.source_url:
            return False
        
        # ã‚¹ã‚³ã‚¢ç¯„å›²ãƒã‚§ãƒƒã‚¯
        if self.relevance_score is not None:
            if not (0 <= self.relevance_score <= 100):
                return False
        
        # æ—¥ä»˜ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if self.published_at > datetime.now():
            # æœªæ¥ã®æ—¥ä»˜ã¯ä¸æ­£
            return False
        
        if self.fetched_at < self.published_at:
            # å–å¾—æ—¥æ™‚ < å…¬é–‹æ—¥æ™‚ ã¯ä¸æ­£
            return False
        
        return True
```

---

## ğŸ“Š ã‚¹ã‚­ãƒ¼ãƒé–¢ä¿‚å›³

```
ã€å…¥åŠ›ã€‘è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹
  â”œâ”€ NewsAPIï¼ˆREST APIï¼‰
  â”œâ”€ RSS ãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆXMLï¼‰
  â”œâ”€ EDINET APIï¼ˆJSONï¼‰
  â””â”€ arXiv APIï¼ˆXMLï¼‰

          â†“
          
ã€æ­£è¦åŒ–ã€‘å„ã‚½ãƒ¼ã‚¹åˆ¥ã®å¤‰æ›å‡¦ç†
  â”œâ”€ NewsAPISource.normalize()
  â”œâ”€ RSSSource.normalize()
  â”œâ”€ EDINETSource.normalize()
  â””â”€ ArxivSource.normalize()

          â†“
          
ã€çµ±ä¸€ã€‘UniversalArticle
  â””â”€ id, title, source_url, ...
  
          â†“
          
ã€ä»¥é™ã®å‡¦ç†ã€‘çµ±ä¸€çš„ã«æ‰±ãˆã‚‹
  â”œâ”€ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
  â”œâ”€ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
  â”œâ”€ è¦ç´„ç”Ÿæˆ
  â”œâ”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
  â””â”€ ...ã™ã¹ã¦åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
  
          â†“
          
ã€å‡ºåŠ›ã€‘è¤‡æ•°å½¢å¼
  â”œâ”€ HTML
  â”œâ”€ Markdown
  â”œâ”€ Notion
  â””â”€ Note
```

---

## ğŸ¯ å®Ÿè£…ä¸Šã®æ³¨æ„ç‚¹

### **1. æ—¥æ™‚ã®æ‰±ã„**
```python
# âŒ é–“é•ã„ï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒãªã„ï¼‰
published_at = datetime.fromisoformat("2026-01-29T08:00:00")

# âœ… æ­£ã—ã„ï¼ˆUTC ã‚’æŒ‡å®šï¼‰
from datetime import timezone
published_at = datetime.fromisoformat("2026-01-29T08:00:00Z".replace('Z', '+00:00'))
published_at = published_at.astimezone(timezone.utc)
```

### **2. ID ã®ç”Ÿæˆ**
```python
# âŒ é–“é•ã„ï¼ˆæ¯å›ç•°ãªã‚‹ ID ã«ãªã£ã¦ã—ã¾ã†ï¼‰
id = str(uuid.uuid4())

# âœ… æ­£ã—ã„ï¼ˆåŒã˜è¨˜äº‹ãªã‚‰åŒã˜ ID ã«ãªã‚‹ï¼‰
id = str(uuid.uuid5(
    uuid.NAMESPACE_DNS,
    f"{title}-{source_name}"
))
```

### **3. JSON ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰**
```python
# keywords ã‚„ authors ã¯ JSON ã§ä¿å­˜
import json

# ä¿å­˜æ™‚ï¼š
keywords_json = json.dumps(["AI", "ä¼æ¥­"])  # ["AI", "ä¼æ¥­"] â†’ '"[""AI"", ""ä¼æ¥­""]'

# èª­ã¿è¾¼ã¿æ™‚ï¼š
keywords = json.loads(keywords_json)  # ["AI", "ä¼æ¥­"]
```

---

## âœ¨ å®Œæˆæ™‚ã®çŠ¶æ…‹

```python
# ã“ã®ã‚ˆã†ã«ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã‚‹

articles = fetch_from_multiple_sources()  # è¤‡æ•°ã‚½ãƒ¼ã‚¹å–å¾—
normalized = normalize_articles(articles)  # æ­£è¦åŒ–

# ä»¥ä¸‹ã¯åŒã˜å‡¦ç†ã§ OK
for article in normalized:
    score = calculate_score(article)      # ã‚¹ã‚³ã‚¢è¨ˆç®—
    summary = generate_summary(article)   # è¦ç´„ç”Ÿæˆ
    keywords = extract_keywords(article)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
```

---

**æ¬¡ã¯ IMPLEMENTATION_PLAN.md ã‚’èª­ã‚“ã§ãã ã•ã„ï¼**
